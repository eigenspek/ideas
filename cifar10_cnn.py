# Introduction to Image classification with a convolutional neural network. It will be trained on a CIFAR-10 Dataset. 

# first, we generate the backend : 
from neon.backends import gen_backend
be = gen_backend(backend='cpu', batch_size = 128)
print be #here we see that we have a CPU backend 

# Now we load the CIFAR-10 dataset,which consists of 60,000 images, each 32x32pixels and 3color channel, with each image belonging 
# to 10 classes : airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.

#The dataset will be returned as numpy arrays containing with pixel values and class labels. The stock datasets are encapsuled in 
# Dataset class objects containing all metadata required to access data. 

from neon.data import CIFAR10
cifar10_dataset = CIFAR10()

# We put the dataset into a format neon can understand by creating an ArrayIterator instance. Doing so moves the data onto the
# compute device (e.g. GPU or CPU) and provides an iterator that returns training minibatches. The data iterators are generated by 
# the DataSet class and the training and validation sets can be access through the attributes train_iter and valid_iter, 
# respectively.
train_set = cifar10_dataset.train_iter
test_set = cifar10_dataset.valid_iter

# Now we need to build layers. Neon can provides different types of layers, such as Linear Convolution, Bias, Activation and 
# Pooling. For commun uses, neon provides shortcuts : 
# 		Conv = Convolution + Bias + Activation 
#		Affine = Linear + Bias + Activation
# The network we build here will be 2Conv->2Pool-->2affine
# assembled all together in a list : 
from neon.layers import Conv, Affine, Pooling
from neon.initializers import Uniform
from neon.transforms.activation import Rectlin, Softmax
init_uni = Uniform(low=-0.1, high=0.1)
layers = [Conv(fshape=(5,5,16), init=init_uni, activation=Rectlin()),
          Pooling(fshape=2, strides=2),
          Conv(fshape=(5,5,32), init=init_uni, activation=Rectlin()),
          Pooling(fshape=2, strides=2),
          Affine(nout=500, init=init_uni, activation=Rectlin()),
          Affine(nout=10, init=init_uni, activation=Softmax())]
# Each convolution layer has a filter size set by the parameter fshape, which should be a tuple (width, height, # of filters).
# The final Affine layer has 10 hidden units, corresponding to the 10 categories in the dataset.

# Now we setup the model : 
from neon.models import Model
model = Model(layers)

# For the cost function we will use the cross-entropy function : 
from neon.layers import GeneralizedCost
from neon.transforms import CrossEntropyMulti
cost = GeneralizedCost(costfunc=CrossEntropyMulti())

# for the optimizer we use the SGD 
from neon.optimizers import GradientDescentMomentum, RMSProp
optimizer = GradientDescentMomentum(learning_rate=0.005, momentum_coef=0.9)

# We define also a callback : 
from neon.callbacks.callbacks import Callbacks
callbacks = Callbacks(model, train_set)

# No we have everything built, we train our model : 
model.fit(dataset=train_set, cost=cost, optimizer=optimizer,  num_epochs=5, callbacks=callbacks)
# AND THIS IS IT ! :D

#Now we just need to evaluate the performance of our model : 
from neon.transforms import Misclassification
error_pct = 100 * model.eval(test_set, metric=Misclassification())
print 'Misclassification error = %.1f%%' % error_pct
accuracy_fp = 100 - error_pct
print 'We thus have an accuracy of %.1f%%' % accuracy_fp 
#############################################################################################################################

# Now we want to import an image from the web and classify it through our model 

# we import the image of a frog from Wikipedia : 
img_source = "https://upload.wikimedia.org/wikipedia/commons/thumb/5/55/Atelopus_zeteki1.jpg/440px-Atelopus_zeteki1.jpg"

# download the image
import urllib
urllib.urlretrieve(img_source, filename="image.jpg")

# crop and resize to 32x32
from PIL import Image
import numpy as np

img = Image.open('image.jpg')
crop = img.crop((0,0,min(img.size),min(img.size)))
crop.thumbnail((32, 32))
crop = np.asarray(crop, dtype=np.float32)

# We create an iterator with this image for inference. Because the models buffers are already initialized with a minibatch
# of size 128, we fill the rest with zeros.	
from neon.data import ArrayIterator
import numpy as np

x_new = np.zeros((128,3072), dtype=np.float32)
x_new[0] = crop.reshape(1,3072)/ 255

inference_set = ArrayIterator(x_new, None, nclass=nclass, lshape=(3, 32, 32))

# Now we get model outputs on the inference data : 
classes =["airplane", "automobile", "bird", "cat", "deer",
          "dog", "frog", "horse", "ship", "truck"]
out = model.get_outputs(inference_set)
print classes[out[0].argmax()]

